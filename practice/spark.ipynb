{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://ihongss.com/webboard/editor?mode=v"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "k20191029 / p20191029\n",
    "\n",
    "=========================== hadoop설치하기 ===========================\n",
    "$ cd /home/ihongss\n",
    "$ sudo apt update\n",
    "\n",
    "자바설치\n",
    "$ sudo apt install openjdk-8-jre-headless\n",
    "$ sudo apt install openjdk-8-jdk-headless\n",
    "\n",
    "$　update-alternatives --display java　=> 자바설치 위치확인\n",
    "   => /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
    "   => /usr/lib/jvm/java-8-openjdk-amd64/bin/java\n",
    "\n",
    "다운로드\n",
    "$ sudo apt install wget\n",
    "$ sudo wget http://apache.tt.co.kr/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\n",
    "$ tar -zxvf hadoop(ESC!!)   => 압축풀기\n",
    "$ mv hadoop-3.1.3 hadoop    => 폴더명 변경\n",
    "\n",
    "$ sudo apt install vim\n",
    "\n",
    "$ vim ~/.bashrc\n",
    "//위쪽에 추가함\n",
    "export HADOOP_HOME=/home/ihongss/hadoop\n",
    "export HADOOP_COMMON_HOME=/home/ihongss/hadoop\n",
    "export HDFS_NAMENODE_USER=\"ihongss\"\n",
    "export HDFS_DATANODE_USER=\"ihongss\"\n",
    "export HDFS_SECONDARYNAMENODE_USER=\"ihongss\"\n",
    "export YARN_RESOURCEMANAGER_USER=\"ihongss\"\n",
    "export YARN_NODEMANAGER_USER=\"ihongss\"\n",
    "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin\n",
    "\n",
    "$ source ~/.bashrc\n",
    "\n",
    "$ hadoop version\n",
    "\n",
    "$ vim ./hadoop/etc/hadoop/hadoop-env.sh\n",
    "    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "\n",
    "$ vim ./hadoop/etc/hadoop/core-site.xml\n",
    "    <configuration>\n",
    "        <property>\n",
    "            <name>fs.defaultFS</name>\n",
    "            <value>hdfs://127.0.0.1:9000</value>\n",
    "        </property>\n",
    "    </configuration>\n",
    "\n",
    "$ vim ./hadoop/etc/hadoop/hdfs-site.xml\n",
    "    <configuration>\n",
    "        <property>\n",
    "            <name>dfs.namenode.name.dir</name>\n",
    "            <value>/home/ihongss/hadoop/data/nameNode</value>\n",
    "        </property>\n",
    "\n",
    "        <property>\n",
    "            <name>dfs.datanode.name.dir</name>\n",
    "            <value>/home/ihongss/hadoop/data/dataNode</value>\n",
    "        </property>       \n",
    "\n",
    "        <property>\n",
    "            <name>dfs.replication</name>\n",
    "            <value>1</value>\n",
    "        </property>       \n",
    "    </configuration>\n",
    "\n",
    "$ vim ./hadoop/etc/hadoop/mapred-site.xml\n",
    "    <configuration>\n",
    "        <property>\n",
    "            <name>map.framework.name</name>\n",
    "            <value>yarn</value>\n",
    "        </property>\n",
    "    </configuration>\n",
    "\n",
    "\n",
    "$ vim ./hadoop/etc/hadoop/yarn-site.xml\n",
    "    <configuration>\n",
    "        <property>\n",
    "            <name>yarn.nodemanager.aux-services</name>\n",
    "            <value>mapreduce_shuffle</value>\n",
    "        </property>\n",
    "    </configuration>\n",
    "\n",
    "$ mkdir -p /home/ihongss/hadoop/data/nameNode  <= 폴더 생성\n",
    "$ mkdir -p /home/ihongss/hadoop/data/dataNode  <= 폴더 생성\n",
    "\n",
    "\n",
    "암호없이 서버에 접속하기 위한 용도   \n",
    "$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
    "$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "$ chmod 0600 ~/.ssh/authorized_keys\n",
    "\n",
    "$ sudo apt install ssh\n",
    "$ sudo service ssh start\n",
    "$ ssh localhost => 연결 확인 (암호 입력이 나오면 안됨)\n",
    "exit로 빠져나옴   \n",
    "\n",
    "$ sudo ufw enable\n",
    "$ sudo ufw allow 9870  => 크롬확인용\n",
    "$ sudo ufw allow 9000 \n",
    "$ sudo ufw allow 9866\n",
    "$ sudo ufw allow 9864\n",
    "$ sudo ufw status   (열린 포트 확인)\n",
    "\n",
    "$ hdfs namenode -format <= name노드 포멧\n",
    "\n",
    "$ start-dfs.sh            <= 구동\n",
    "$ jps\n",
    "   NameNode,\n",
    "   DataNode,\n",
    "   SecondaryNameNode,\n",
    "\n",
    "$ start-yarn.sh      \n",
    "$ jps\n",
    "    NodeManager,\n",
    "    ResourceMansger\n",
    "\n",
    "$ stop-dfs.sh\n",
    "\n",
    "$ stop-yarn.sh\n",
    "\n",
    "* nameNode의 파일과 dataNode파일을 지우고 hdfs namenode -format 명령어로 포맷 후에 재구동\n",
    "\n",
    "\n",
    "===========spark 설치하기============================\n",
    "* 하둡이 설치되어 있어야 함.\n",
    "\n",
    "$ pwd  <= 현재 위치 보기\n",
    "    /home/team1\n",
    "\n",
    "$ rm -rf  spark-2.4.4-bin*    <= 기존 파일 지우기\n",
    "\n",
    "$ wget http://apache.tt.co.kr/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz  <= 다운로드\n",
    "\n",
    "$ tar -xzvf spark-2.4.4-bin-hadoop2.7.tgz <= 압축풀기\n",
    "\n",
    "$ mv spark-2.4.4-bin-hadoop2.7 spark  <= 폴더명 변경\n",
    "\n",
    "$ sudo apt install scala   <= scala 설치\n",
    "\n",
    "$ vim ~/.bashrc               <= 환경설정\n",
    "    export SPARK_HOME=/home/team1/spark\n",
    "    export PATH=기존것:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "\n",
    "$ source ~/.bashrc            <= 환경설정 적용\n",
    "\n",
    "$ cd /home/team1/spark/conf   <= 폴더 이동\n",
    "\n",
    "$ cp spark-env.sh.template spark-env.sh  <= 파일복사\n",
    "\n",
    "$ vim ./spark-env.sh  <= 파일편집\n",
    "    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "    export HADOOP_HOME=/home/team5/hadoop\n",
    "    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
    "    export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native\n",
    "    export SPARK_LOCAL_IP=\"127.0.0.1\"\n",
    "    export SPARK_MASTER_HOST=\"127.0.0.1\"\n",
    "\n",
    "$ cp spark-defaults.conf.template spark-defaults.conf\n",
    "\n",
    "$ vim ./spark-defaults.conf\n",
    "   spark.master   spark://127.0.0.1:7077\n",
    "\n",
    "$ start-master.sh  <=  spark 실행\n",
    "\n",
    "$ jps\n",
    "    13937 DataNode\n",
    "    21893 Master         <= 추가된 프로세스\n",
    "    13766 NameNode\n",
    "    14455 ResourceManager\n",
    "    14183 SecondaryNameNode\n",
    "    14623 NodeManager\n",
    "    22127 Jps\n",
    "\n",
    "$spark-shell\n",
    "    :quit   <= 종료\n",
    "    \n",
    "====zeppelin 설치============\n",
    "\n",
    "$ cd ~\n",
    "$ pwd  => /home/team1\n",
    "\n",
    "$ wget http://apache.mirror.cdnetworks.com/zeppelin/zeppelin-0.8.1/zeppelin-0.8.1-bin-all.tgz  <= 파일 받기\n",
    "\n",
    "$ tar xf zeppelin-*-bin-all.tgz  <= 압축풀기\n",
    "\n",
    "$ mv zeppelin-0.8.1-bin-all zeppelin  <= 폴더명 변경\n",
    "\n",
    "$ cd zeppelin/conf   <= 폴더 이동\n",
    "\n",
    "$ cp zeppelin-env.sh.template zeppelin-env.sh  <= 파일 복사\n",
    "\n",
    "\n",
    "$ which python3\n",
    "    /파이썬 경로\n",
    "\n",
    "$ vim zeppelin-env.sh\n",
    "    export SPARK_HOME=/home/team1/spark\n",
    "    export PYTHONPATH=/home/team1/spark/python\n",
    "    export PYSPARK_PYTHON=/위에서 나오는 파이썬 경로\n",
    "    \n",
    "$ cp zeppelin-site.xml.template zeppelin-site.xml    \n",
    "\n",
    "$ vim zeppelin-site.xml\n",
    "    <property>\n",
    "        <name>zeppelin.server.addr</name>\n",
    "        <value>127.0.0.1</value>\n",
    "        <description>Server address</description>\n",
    "    </property>\n",
    "    \n",
    "    <property>\n",
    "        <name>zeppelin.server.port</name>\n",
    "        <value>9999</value>\n",
    "        <description>Server port.</description>\n",
    "    </property>\n",
    "\n",
    "\n",
    "암호 설정\n",
    "$ cd /home/team1/zeppelin/conf \n",
    "$ ls\n",
    "    shiro.ini.template 파일이 있는지 확인\n",
    "$ cp shiro.ini.template shiro.ini   <= 파일 복사\n",
    "\n",
    "$ vim shiro.ini\n",
    "    admin=password1, admin   <= \"#\" 주석 제거함\n",
    "    user1=password2, role1, role2\n",
    "    user2=password3, role3\n",
    "    user3=password4, role2\n",
    "    :wq  <= 저장하고 종료하기\n",
    "\n",
    "$ vim zeppelin-site.xml\n",
    "<property>\n",
    "   <name>zeppelin.anonymous.allowed</name>\n",
    "   <value>false</value>   <= 익명의 사용자 접속 불가로 변경\n",
    "   <description>Anonymous user allowed by default</description>\n",
    "</property>\n",
    "\n",
    "$ vim ~/.bashrc\n",
    "    export ZEPPELIN_HOME=/home/team1/zeppelin\n",
    "    [ 생략 ]\n",
    "    export PATH=기존것:$ZEPPELIN_HOME/bin    <= 기존 내용에서 추가 \n",
    "\n",
    "$ source ~/.bashrc\n",
    "\n",
    "$ zeppelin.sh =>  최초 실행 ( 실행결과 모니터링 )\n",
    "\n",
    "$ zeppelin-daemon.sh start => 서비스 구동\n",
    "\n",
    "$ zeppelin-daemon.sh stop => 서비스 중지\n",
    "\n",
    "\n",
    "크롬에서 :127.0.0.1:9999 엔터\n",
    "\n",
    "하둡 파일관리\n",
    "$ hdfs dfs -ls /    <= HDFS 파일의 목록 확인\n",
    "$ hdfs dfs -mkdir / <= HDFS에 폴더 생성\n",
    "$ cd ~\n",
    "$ vim a.txt\n",
    "    fkj fekljfe fekljl eflkjfe fejlpfejl\n",
    "    :wq\n",
    "\n",
    "$ hdfs dfs -put a.txt /test1    <= a.txt 파일을  HDFS의  /test1  폴더에 복사\n",
    "\n",
    "\n",
    "예제) HDFS 파일 읽기 ===============================\n",
    "# CSV파일 읽기\n",
    "%spark.pyspark  \n",
    "\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "\"\"\"\n",
    "InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country  <= 컬럼명\n",
    "581475,22596,CHRISTMAS STAR WISH LIST CHALKBOARD,36,2011-12-09 08:39:00,0.39,13069.0,United Kingdom  <= 데이터\n",
    "\"\"\"\n",
    "\n",
    "schema = StructType().add(\"InvoiceNo\", StringType(), True) \\\n",
    "    .add(\"StockCode\", StringType(), True) \\\n",
    "    .add(\"InvoiceNo\", StringType(), True) \\\n",
    "    .add(\"InvoiceNo\", StringType(), True) \\\n",
    "    .add(\"InvoiceNo\", StringType(), True) \\\n",
    "    .add(\"InvoiceNo\", StringType(), True) \\\n",
    "    .add(\"InvoiceNo\", StringType(), True) \\\n",
    "    .add(\"InvoiceNo\", StringType(), True) \n",
    "    \n",
    "\n",
    "df=spark.read.option(\"header\", \"true\") \\\n",
    "    .schema(schema).csv('hdfs://175.126.73.18:9000/test1/exam2.csv')\n",
    "df.printSchema\n",
    "df.show()\n",
    "print(df)\n",
    "\n",
    "# pandas로 변경\n",
    "pd1 = df.toPandas()\n",
    "\n",
    "# sql문 사용\n",
    "df.createOrReplaceTempView(\"df_table\")\n",
    "spark.sql(\"SELECT * FROM df_table WHERE Quantity>=20\").show()\n",
    "\n",
    "\n",
    "\n",
    "예제) DataFrame 생성 ===============================\n",
    "%spark.pyspark\n",
    "\n",
    "# create DataFrame from python list. It can infer schema for you.\n",
    "df1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "df1.printSchema\n",
    "df1.show()\n",
    "\n",
    "# create DataFrame from pandas dataframe\n",
    "df2 = spark.createDataFrame(df1.toPandas())\n",
    "df2.printSchema\n",
    "df2.show()\n",
    "\n",
    "\n",
    "\n",
    "===원격으로 접속=================================\n",
    "1. java설치\n",
    "    C:\\Program Files\\Java\\jdk1.8.0_211\n",
    "    \n",
    "2. winutils  다운로드 : https://github.com/steveloughran/winutils  \n",
    "    C:\\hadoop-3.0.0\n",
    "\n",
    "3. 시스템 변수 등록 => vim ~/.bashrc\n",
    "    JAVA_HOME       C:\\Program Files\\Java\\jdk1.8.0_211  \n",
    "    HADOOP_HOME     C:\\hadoop-3.0.0\n",
    "\n",
    "4. 라이브러리 설치\n",
    "    pip install pyspark\n",
    "    conda install pyspark\n",
    "\n",
    "5. 소스 작성\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"spark_app1\").getOrCreate()\n",
    "\n",
    "#spark = SparkSession.builder.master(\"spark://172.30.32.173:7077\").appName(\"spark_app1\").getOrCreate()\n",
    "\n",
    "df1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "df1.printSchema\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-------+\n",
      "| id| name|age|country|\n",
      "+---+-----+---+-------+\n",
      "|  1| andy| 20|    USA|\n",
      "|  2| jeff| 23|  China|\n",
      "|  3|james| 18|    USA|\n",
      "+---+-----+---+-------+\n",
      "\n",
      "+---+-----+---+-------+\n",
      "| id| name|age|country|\n",
      "+---+-----+---+-------+\n",
      "|  3|james| 18|    USA|\n",
      "+---+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"spark_app1\").getOrCreate()\n",
    "# spark = SparkSession.builder.master(\"spark://172.30.32.173:7077\").appName(\"spark_app1\").getOrCreate()\n",
    "\n",
    "df1 = spark.createDataFrame([(1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "df1.printSchema\n",
    "df1.show()\n",
    "\n",
    "df1.createOrReplaceTempView(\"df_table\")\n",
    "spark.sql(\"SELECT * FROM df_table WHERE age < 20\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+-----------------+\n",
      "|height|weight|age|gender|         features|\n",
      "+------+------+---+------+-----------------+\n",
      "| 160.0|  65.3| 40|   1.0|[160.0,65.3,40.0]|\n",
      "| 166.0|  35.3| 20|   0.0|[166.0,35.3,20.0]|\n",
      "| 180.0|  65.3| 40|   1.0|[180.0,65.3,40.0]|\n",
      "+------+------+---+------+-----------------+\n",
      "\n",
      "+------+------+---+------+-----------------+--------------------+--------------------+----------+\n",
      "|height|weight|age|gender|         features|       rawPrediction|         probability|prediction|\n",
      "+------+------+---+------+-----------------+--------------------+--------------------+----------+\n",
      "| 160.0|  65.3| 40|   1.0|[160.0,65.3,40.0]|[-4.7251940334141...|[0.00879102532117...|       1.0|\n",
      "| 166.0|  35.3| 20|   0.0|[166.0,35.3,20.0]|[3.17760255117735...|[0.95998266728207...|       0.0|\n",
      "| 180.0|  65.3| 40|   1.0|[180.0,65.3,40.0]|[-3.2900693199985...|[0.03591344569278...|       1.0|\n",
      "+------+------+---+------+-----------------+--------------------+--------------------+----------+\n",
      "\n",
      "+------+------+---+------+-----------------+--------------------+--------------------+----------+\n",
      "|height|weight|age|gender|         features|       rawPrediction|         probability|prediction|\n",
      "+------+------+---+------+-----------------+--------------------+--------------------+----------+\n",
      "| 160.0|  65.3| 40|   1.0|[160.0,65.3,40.0]|[-4.7251940334141...|[0.00879102532117...|       1.0|\n",
      "| 166.0|  35.3| 20|   0.0|[166.0,35.3,20.0]|[3.17760255117735...|[0.95998266728207...|       0.0|\n",
      "| 180.0|  65.3| 40|   1.0|[180.0,65.3,40.0]|[-3.2900693199985...|[0.03591344569278...|       1.0|\n",
      "+------+------+---+------+-----------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1059.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:441)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelWriter.saveImpl(LogisticRegression.scala:1236)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:180)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 88.0 failed 1 times, most recent failure: Lost task 0.0 in stage 88.0 (TID 305, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\data\\spark\\regression_model\\metadata\\_temporary\\0\\_temporary\\attempt_20191030163322_0262_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 42 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\data\\spark\\regression_model\\metadata\\_temporary\\0\\_temporary\\attempt_20191030163322_0262_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-46c4323b499d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# 모델 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mpath1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C:/data/spark/regression_model\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 쓰기\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[0mloadModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#읽기\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1059.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:441)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelWriter.saveImpl(LogisticRegression.scala:1236)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:180)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 88.0 failed 1 times, most recent failure: Lost task 0.0 in stage 88.0 (TID 305, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\data\\spark\\regression_model\\metadata\\_temporary\\0\\_temporary\\attempt_20191030163322_0262_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 42 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\data\\spark\\regression_model\\metadata\\_temporary\\0\\_temporary\\attempt_20191030163322_0262_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# ML 예제\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('ml_log').master('local[*]').getOrCreate()\n",
    "\n",
    "# 훈련용 데이터(키, 몸무게, 나이, 성별)\n",
    "train = spark.createDataFrame([(160.0, 65.3, 40, 1.0), \n",
    "                               (166.0, 35.3, 20, 0.0), \n",
    "                               (180.0, 65.3, 40, 1.0)]).toDF('height', 'weight', 'age', 'gender')\n",
    "# 테스트용(키, 몸무게, 나이)\n",
    "test = spark.createDataFrame([(160.0, 56.5, 34)]).toDF('height', 'weight', 'age')\n",
    "\n",
    "# vector assemble (1번 항목)\n",
    "assemble = VectorAssembler(inputCols = ['height', 'weight', 'age'], outputCol='features')\n",
    "\n",
    "# train 데이터에 features 추가\n",
    "assembled_train = assemble.transform(train)\n",
    "assembled_train.show()\n",
    "\n",
    "# 모델 생성 알고리즘 (2번 항목)\n",
    "lr = LogisticRegression(maxIter = 10, regParam = 0.01, labelCol = 'gender')\n",
    "\n",
    "# 모델 생성\n",
    "model = lr.fit(assembled_train)\n",
    "\n",
    "# 예측값 확인\n",
    "model.transform(assembled_train).show()\n",
    "\n",
    "# 파이프라인\n",
    "pipeline = Pipeline(stages = [assemble, lr])\n",
    "\n",
    "# 파이프라인 모델 생성\n",
    "pipelineModel = pipeline.fit(train)\n",
    "\n",
    "# 파이프라인을 이용한 예측값\n",
    "pipelineModel.transform(train).show()\n",
    "\n",
    "# 모델 저장\n",
    "path1=\"C:/data/spark/regression_model\"\n",
    "model.write().overwrite().save(path1)  # 쓰기\n",
    "loadModel = LogisticRegressionModel.load(path1) #읽기 \n",
    "\n",
    "# 파이프라인 모델 저장\n",
    "path2=\"c:/data/spark/pipeline_model\"\n",
    "pipelineModel.write().overwrite().save(path2)  # 쓰기\n",
    "loadpipeModel = PipelineModel.load(path2)    # 읽기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
