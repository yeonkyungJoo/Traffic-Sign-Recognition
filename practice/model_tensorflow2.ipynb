{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from collections import namedtuple\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클래스(label) 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['70', 'no_parking', 'no_parking_stop', '50', '60', '30']\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for folder in glob.glob('./data/*') :\n",
    "    label = folder.split('/')[-1]\n",
    "    # print(label)\n",
    "    labels.append(label)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 개수\n",
    "N_CLASSES = len(labels)\n",
    "# input image size\n",
    "RESIZED_IMAGE = (28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cnt = []\n",
    "for label in labels :\n",
    "    # print('./data/{}/*.jpg'.format(label))\n",
    "    cnt = 0\n",
    "    for file in glob.glob('./data/{}/*.jpg'.format(label)) :\n",
    "        cnt += 1\n",
    "    img_cnt.append(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[660, 189, 304, 750, 480, 730]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스별 이미지 개수\n",
    "labels_cnt = {}\n",
    "for label in labels :\n",
    "    cnt = 0\n",
    "    for file in glob.glob('./data/{}/*.jpg'.format(label)) :\n",
    "        cnt += 1\n",
    "    labels_cnt[label] = cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'70': 660,\n",
       " 'no_parking': 189,\n",
       " 'no_parking_stop': 304,\n",
       " '50': 750,\n",
       " '60': 480,\n",
       " '30': 730}"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {}\n",
    "for label, i in zip(labels, range(0, N_CLASSES)) :\n",
    "    LABELS[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'70': 0, 'no_parking': 1, 'no_parking_stop': 2, '50': 3, '60': 4, '30': 5}"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_IDX = {}\n",
    "for i, label in zip(range(0, N_CLASSES), labels) :\n",
    "    LABELS_IDX[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '70', 1: 'no_parking', 2: 'no_parking_stop', 3: '50', 4: '60', 5: '30'}"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS_IDX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "Dataset = namedtuple('Dataset', ['X', 'y'])\n",
    "print(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우에 맞는 이미지 텐서 포맷 : [?, ?, ?, ?]\n",
    "# 차원 확장\n",
    "def to_tf_format(imgs) :\n",
    "    return np.stack([img[:, :, np.newaxis] for img in imgs], axis = 0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 경로에 주어진 모든 이미지를 읽어서\n",
    "# 그 이미지를 미리 정의한 형상으로 크기 재조정\n",
    "# 회색조로 변환\n",
    "# 레이블 원-핫 인코딩\n",
    "\n",
    "def read_dataset(root_path, labels_arr, resize_to) :\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for label_name in labels_arr :\n",
    "        # 'data/no_parking/'\n",
    "        full_path = root_path + '/' + label_name + '/'\n",
    "        \n",
    "        for img in glob.glob(full_path + '*.jpg') :\n",
    "            image = cv2.imread(img).astype(np.float32)\n",
    "            # 회색조로 변환\n",
    "            image = cv2.cvtColor(image, cv2.IMREAD_GRAYSCALE)[:, :, 0]\n",
    "            # 크기 조정\n",
    "            if resize_to : \n",
    "                image = cv2.resize(image, resize_to)\n",
    "            \n",
    "            # 라벨\n",
    "            # 원-핫 인코딩\n",
    "            label = np.zeros((N_CLASSES, ), dtype = np.float32)\n",
    "            label[LABELS[label_name]] = 1.0\n",
    "            \n",
    "            images.append(image.astype(np.float32))\n",
    "            labels.append(label)\n",
    "            \n",
    "    return Dataset(X = to_tf_format(images).astype(np.float32), y = np.matrix(labels).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3113, 28, 28, 1)\n",
      "(3113, 6)\n"
     ]
    }
   ],
   "source": [
    "dataset = read_dataset('data', labels, RESIZED_IMAGE)\n",
    "print(dataset.X.shape)\n",
    "print(dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3113, 28, 28, 1) (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.X.shape, dataset.X[0, :, :, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "print((dataset.X[0, :, :, :].reshape(RESIZED_IMAGE)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 첫번째 표본\n",
    "plt.imshow(dataset.X[0, :, :, :].reshape(RESIZED_IMAGE))\n",
    "# 레이블\n",
    "print(dataset.y[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 마지막 표본\n",
    "plt.imshow(dataset.X[-1, :, :, :].reshape(RESIZED_IMAGE))\n",
    "# 레이블\n",
    "print(dataset.y[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test 데이터셋 분류\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "idx_train, idx_test = train_test_split( range(dataset.X.shape[0]), test_size = 0.25, random_state = 101 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2334, 779)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_train), len(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset.X[idx_train, :, :, :]\n",
    "X_test = dataset.X[idx_test, :, :, :]\n",
    "y_train = dataset.y[idx_train, :]\n",
    "y_test = dataset.y[idx_test, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0.]], dtype=float32), 2334)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0.]], dtype=float32), 779)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test, len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2334, 28, 28, 1)\n",
      "(779, 28, 28, 1)\n",
      "(2334, 6)\n",
      "(779, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터의 미니배치 생성\n",
    "\n",
    "def minibatcher(X, y, batch_size, shuffle) :\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    if shuffle :\n",
    "        idx = np.random.permutation(n_samples)\n",
    "    else :\n",
    "        idx = list(range(n_samples))\n",
    "        \n",
    "    for k in range(int(np.ceil(n_samples / batch_size))) :\n",
    "        from_idx = k * batch_size\n",
    "        to_idx = (k+1) * batch_size\n",
    "        yield X[idx[from_idx:to_idx], :, :, :], y[idx[from_idx:to_idx], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28, 1) (1000, 6)\n",
      "(1000, 28, 28, 1) (1000, 6)\n",
      "(334, 28, 28, 1) (334, 6)\n"
     ]
    }
   ],
   "source": [
    "for mb in minibatcher(X_train, y_train, 1000, True) :\n",
    "    print(mb[0].shape, mb[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer : AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 클래스 생성\n",
    "\n",
    "class Model :\n",
    "    \n",
    "    def __init__(self, sess, name) :\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "        \n",
    "    def _build_net(self) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "\n",
    "            # dropout rate\n",
    "            # 0.7 ~ 0.5 on training, but should be 1 for testing\n",
    "            # self.keep_prob = tf.placeholder(tf.float32)\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "\n",
    "            # input placeholders\n",
    "            self.X = tf.placeholder(tf.float32, shape = (None, RESIZED_IMAGE[0], RESIZED_IMAGE[1], 1))\n",
    "            self.y = tf.placeholder(tf.float32, shape = (None, N_CLASSES))\n",
    "\n",
    "            # Layer1\n",
    "            conv1 = tf.layers.conv2d(inputs = self.X, filters = 32, kernel_size = [3, 3], padding = 'SAME', activation = tf.nn.relu)\n",
    "            pool1 = tf.layers.max_pooling2d(inputs = conv1, pool_size = [2, 2], padding = 'SAME', strides = 2)\n",
    "            dropout1 = tf.layers.dropout(inputs = pool1, rate = 0.3, training = self.training)\n",
    "\n",
    "            # Layer2\n",
    "            conv2 = tf.layers.conv2d(inputs = dropout1, filters = 64, kernel_size = [3, 3], padding = 'SAME', activation = tf.nn.relu)\n",
    "            pool2 = tf.layers.max_pooling2d(inputs = conv2, pool_size = [2, 2], padding = 'SAME', strides = 2)\n",
    "            dropout2 = tf.layers.dropout(inputs = pool2, rate = 0.3, training = self.training)\n",
    "\n",
    "            # Layer3\n",
    "            conv3 = tf.layers.conv2d(inputs = dropout2, filters = 128, kernel_size = [3, 3], padding = 'SAME', activation = tf.nn.relu)\n",
    "            pool3 = tf.layers.max_pooling2d(inputs = conv3, pool_size = [2, 2], padding = 'SAME', strides = 2)\n",
    "            dropout3 = tf.layers.dropout(inputs = pool3, rate = 0.3, training = self.training)\n",
    "\n",
    "            # Dense Layer with Relu\n",
    "            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n",
    "            dense4 = tf.layers.dense(inputs = flat, units = 625, activation = tf.nn.relu)\n",
    "            dropout4 = tf.layers.dropout(inputs = dense4, rate = 0.5, training = self.training)\n",
    "\n",
    "            # Logits (no activation) Layer\n",
    "            self.logits = tf.layers.dense(inputs = dropout4, units = N_CLASSES)\n",
    "            # self.sm_logits = tf.nn.softmax(self.logits)\n",
    "\n",
    "            # define cost/loss & optimizer\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.y))\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            \n",
    "    def train(self, X_data, y_data, training = True) :\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict = {self.X : X_data, self.y : y_data, self.training : training})\n",
    "            \n",
    "    def predict(self, X_test, training = False) :\n",
    "        return self.sess.run(self.logits, feed_dict = {self.X : X_test, self.training : training})\n",
    "        \n",
    "    def get_accuracy(self, X_test, y_test, training = False) :\n",
    "        return self.sess.run(self.accuracy, feed_dict = {self.X : X_test, self.y : y_test, self.training : training})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_POINT_DIR = './model.ckpt'\n",
    "if not os.path.exists(CHECK_POINT_DIR) :\n",
    "    os.makedirs(CHECK_POINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa60c054eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa60c054eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa60c054eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa60c054eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6fc641828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6fc641828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa6050f1400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa6050f1400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa6050f1400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa6050f1400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6050f1400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6050f1400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6050f1400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa6050f1400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa6050f1400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa6050f1400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa6050f1400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa6050f1400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "m1 = Model(sess, \"m1\")\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(100, 28, 28, 1) (100, 6)\n",
      "(34, 28, 28, 1) (34, 6)\n"
     ]
    }
   ],
   "source": [
    "for mb in minibatcher(X_train, y_train, batch_size, True) :\n",
    "    print(mb[0].shape, mb[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(max_epochs):\n",
    "#     print(\"Epoch=\", epoch)\n",
    "#     tf_score = []\n",
    "\n",
    "#     for mb in minibatcher(X_train, y_train, batch_size, True) :\n",
    "#         # print(mb)\n",
    "#         tf_output = session.run([optimizer, loss], \n",
    "#                                             feed_dict = {in_X_tensors_batch : mb[0], \n",
    "#                                                               in_y_tensors_batch : mb[1],\n",
    "#                                                               is_training : True})\n",
    "\n",
    "#         tf_score.append(tf_output[1])\n",
    "#     print(\" train_loss_score=\", np.mean(tf_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  0\n",
      "cost =  12.042949\n",
      "Epoch =  1\n",
      "cost =  1.7338486\n",
      "Epoch =  2\n",
      "cost =  1.6838955\n",
      "Epoch =  3\n",
      "cost =  1.6443996\n",
      "Epoch =  4\n",
      "cost =  1.6082834\n",
      "Epoch =  5\n",
      "cost =  1.5707575\n",
      "Epoch =  6\n",
      "cost =  1.5522329\n",
      "Epoch =  7\n",
      "cost =  1.5289235\n",
      "Epoch =  8\n",
      "cost =  1.4855719\n",
      "Epoch =  9\n",
      "cost =  1.472524\n",
      "Epoch =  10\n",
      "cost =  1.4509937\n",
      "Epoch =  11\n",
      "cost =  1.3934907\n",
      "Epoch =  12\n",
      "cost =  1.401502\n",
      "Epoch =  13\n",
      "cost =  1.3533959\n",
      "Epoch =  14\n",
      "cost =  1.3536626\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "for epoch in range(training_epochs) :\n",
    "    print(\"Epoch = \", epoch)\n",
    "    cost = []\n",
    "    # total_batch = int(len(idx_train) / batch_size)\n",
    "    \n",
    "    for mb in minibatcher(X_train, y_train, batch_size, True) :\n",
    "        c, _ = m1.train(mb[0], mb[1])\n",
    "        # avg_cost += c / total_batch\n",
    "        cost.append(c)\n",
    "    # print('Epoch : ', '%04d' % (epoch + 1), 'cost = ', '{:.9f}'.format(avg_cost))\n",
    "    print('cost = ', np.mean(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Model Saved.\n"
     ]
    }
   ],
   "source": [
    "saver.save(sess, CHECK_POINT_DIR + '/model')\n",
    "print('Trained Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.get_checkpoint_state(CHECK_POINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_checkpoint_path: \"./model.ckpt/model\"\n",
      "all_model_checkpoint_paths: \"./model.ckpt/model\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model.ckpt/model\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7613411  -1.5820392  -1.2711114   1.8036005   0.34067672  1.3355687 ]\n",
      " [-0.27635512 -0.31923607 -0.8668401   0.5529812   0.2256749   0.34714302]\n",
      " [-0.19533697 -0.1620249   0.23486826  0.31592825 -0.23467734  0.3810448 ]\n",
      " ...\n",
      " [ 0.3599037  -0.72156346 -0.5375101   0.283992    0.36234605  0.05460368]\n",
      " [ 0.41375402 -0.67032456 -0.40155527  0.14774507  0.44371957  0.04435586]\n",
      " [ 1.0222238  -3.5286634  -3.8428066   1.896579    3.1916013   0.28566262]]\n"
     ]
    }
   ],
   "source": [
    "print(m1.predict(X_test))\n",
    "# argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "779"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m1.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "779"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sess.run(tf.argmax(m1.predict(X_test), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(sess.run(tf.argmax(m1.predict(X_test), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_num = list(sess.run(tf.argmax(m1.predict(X_test), 1)))\n",
    "pred_name = []\n",
    "for num in pred_num :\n",
    "    pred_name.append(LABELS_IDX[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 예측\n",
    "# import random\n",
    "# r = random.randint(0, len(idx_test)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.49422336\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "print('accuracy = ', m1.get_accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼동행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실시간 영상에 분류 결과 나타내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model.ckpt/model.meta\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint.model_checkpoint_path + '.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a trained model\n",
    "saver = tf.train.import_meta_graph(checkpoint.model_checkpoint_path + '.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt/model\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, checkpoint.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(checkpoint)\n",
    "# print(checkpoint.model_checkpoint_path)\n",
    "\n",
    "# if checkpoint and checkpoint.model_checkpoint_path :\n",
    "#     try : \n",
    "#         saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "#         print('Successfully loaded : ', checkpoint.model_checkpoint_path)\n",
    "#     except :\n",
    "#         print(\"Error on loading old network weights\")\n",
    "# else :\n",
    "#     print(\"Could not find old network weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa704268ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa704268ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa704268ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa704268ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa704268ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa704268ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa704268ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa704268ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa704268ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa704268ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa704268ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa704268ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa704268ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa704268ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa704268ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa704268ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6c01d1080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6c01d1080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6c01d1080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa6c01d1080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa704303198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa704303198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa704303198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa704303198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa704303198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa704303198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa704303198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fa704303198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa704303198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa704303198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa704303198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fa704303198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa704303198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa704303198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa704303198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa704303198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5f864a7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5f864a7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5f864a7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5f864a7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa5f864e748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa5f864e748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa5f864e748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fa5f864e748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5f864e748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5f864e748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5f864e748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa5f864e748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "m2 = Model(sess, \"m2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value m2/conv2d/kernel\n\t [[node m2/conv2d/kernel/read (defined at <ipython-input-413-ad51d7a85b4c>:23) ]]\n\nOriginal stack trace for 'm2/conv2d/kernel/read':\n  File \"/home/team2/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/team2/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/team2/anaconda3/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/home/team2/anaconda3/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/home/team2/anaconda3/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2880, in _run_cell\n    return runner(coro)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3248, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3325, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-441-ba14269a5e9e>\", line 1, in <module>\n    m2 = Model(sess, \"m2\")\n  File \"<ipython-input-413-ad51d7a85b4c>\", line 8, in __init__\n    self._build_net()\n  File \"<ipython-input-413-ad51d7a85b4c>\", line 23, in _build_net\n    conv1 = tf.layers.conv2d(inputs = self.X, filters = 32, kernel_size = [3, 3], padding = 'SAME', activation = tf.nn.relu)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/convolutional.py\", line 424, in conv2d\n    return layer.apply(inputs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1479, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/base.py\", line 537, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 591, in __call__\n    self._maybe_build(inputs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1881, in _maybe_build\n    self.build(input_shapes)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 165, in build\n    dtype=self.dtype)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/base.py\", line 450, in add_weight\n    **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 384, in add_weight\n    aggregation=aggregation)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 663, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1496, in get_variable\n    aggregation=aggregation)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1239, in get_variable\n    aggregation=aggregation)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 562, in get_variable\n    aggregation=aggregation)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 514, in _true_getter\n    aggregation=aggregation)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 929, in _get_single_variable\n    aggregation=aggregation)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n    shape=shape)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2511, in default_variable_creator\n    shape=shape)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 1568, in __init__\n    shape=shape)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 1755, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 86, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4253, in identity\n    \"Identity\", input=input, name=name)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value m2/conv2d/kernel\n\t [[{{node m2/conv2d/kernel/read}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-442-7bcf0a3ecef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-413-ad51d7a85b4c>\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(self, X_test, y_test, training)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value m2/conv2d/kernel\n\t [[node m2/conv2d/kernel/read (defined at <ipython-input-413-ad51d7a85b4c>:23) ]]\n\nOriginal stack trace for 'm2/conv2d/kernel/read':\n  File \"/home/team2/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/team2/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/team2/anaconda3/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/home/team2/anaconda3/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/home/team2/anaconda3/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2880, in _run_cell\n    return runner(coro)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3248, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3325, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-441-ba14269a5e9e>\", line 1, in <module>\n    m2 = Model(sess, \"m2\")\n  File \"<ipython-input-413-ad51d7a85b4c>\", line 8, in __init__\n    self._build_net()\n  File \"<ipython-input-413-ad51d7a85b4c>\", line 23, in _build_net\n    conv1 = tf.layers.conv2d(inputs = self.X, filters = 32, kernel_size = [3, 3], padding = 'SAME', activation = tf.nn.relu)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/convolutional.py\", line 424, in conv2d\n    return layer.apply(inputs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1479, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/base.py\", line 537, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 591, in __call__\n    self._maybe_build(inputs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1881, in _maybe_build\n    self.build(input_shapes)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 165, in build\n    dtype=self.dtype)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/base.py\", line 450, in add_weight\n    **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 384, in add_weight\n    aggregation=aggregation)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 663, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1496, in get_variable\n    aggregation=aggregation)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1239, in get_variable\n    aggregation=aggregation)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 562, in get_variable\n    aggregation=aggregation)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 514, in _true_getter\n    aggregation=aggregation)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 929, in _get_single_variable\n    aggregation=aggregation)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n    shape=shape)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2511, in default_variable_creator\n    shape=shape)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 1568, in __init__\n    shape=shape)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 1755, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 86, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4253, in identity\n    \"Identity\", input=input, name=name)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/home/team2/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "print('accuracy = ', sess.run(m2.get_accuracy(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 경로 설정\n",
    "video_path = './video_data/video1.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "video_file = video_path.split('/')[-1]\n",
    "video_name = video_file.split('.')[0]\n",
    "\n",
    "# 커널 생성\n",
    "kernel1 = np.ones((2, 2), np.uint8)\n",
    "kernel2 = np.ones((4, 4), np.uint8)\n",
    "\n",
    "# 프레임 이동\n",
    "# frame_move = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) // 2)\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, frame_move)\n",
    "\n",
    "while cap.isOpened() :\n",
    "    ret, img = cap.read()\n",
    "    \n",
    "    if ret is False :\n",
    "        break\n",
    "        \n",
    "    # 이미지 크기 변경\n",
    "    img = cv2.resize(img, (img.shape[1]//2, img.shape[0]//2), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    ######### 1. 표지판 검출 #########\n",
    "    \n",
    "    ## 1-1. 표지판 ROI 생성\n",
    "    img_sign = np.zeros(img.shape, img.dtype)\n",
    "    img_sign[0 : img.shape[0]*11//20, : ] = img[0 : img.shape[0]*11//20 ,  : ]\n",
    "       \n",
    "    ## 1-2. 붉은색 검출\n",
    "    # 1) 색 공간 변환(BGR2YUV) -> 붉은색 계열 검출\n",
    "    yuv_sign = cv2.cvtColor(img_sign, cv2.COLOR_BGR2YUV)\n",
    "    yuv_sign[yuv_sign[:, :, 2]<135] = 0 # V 채널\n",
    "    yuv_sign[yuv_sign[:, :, 1]<110] = 0 # U 채널\n",
    "    yuv_sign[yuv_sign[:, :, 0]>225] = 0 # 밝기\n",
    "    # V 채널 이진화 \n",
    "    th, sign_v_bin = cv2.threshold(yuv_sign[ : , : , 2], 140, 255, cv2.THRESH_BINARY)\n",
    "    erosion_sign_v_bin = cv2.erode(sign_v_bin, kernel1, iterations = 1) # 침식\n",
    "    dilation_sign_v_bin = cv2.dilate(erosion_sign_v_bin, kernel2, iterations = 1) # 팽창\n",
    "    \n",
    "    # 2) 캐니 엣지 -> 조명 영향 제거\n",
    "    sign_edge = cv2.Canny(img_sign, 100, 200)\n",
    "    dilation_sign_edge = cv2.dilate(edge, kernel1, iterations = 1) # 팽창\n",
    "    erosion_sign_edge = cv2.erode(dilation_edge, kernel1, iterations = 1) # 침식\n",
    "    \n",
    "    sign_red = cv2.bitwise_and(dilation_sign_v_bin, dilation_sign_v_bin, mask = erosion_sign_edge)\n",
    "    \n",
    "    ## 1-3. 원 검출\n",
    "    # 1) 컨투어\n",
    "    # RETR_EXTERNAL / RETR_TREE / RETR_LIST / RETR_CCOMP\n",
    "    _, contours, hierarchy = cv2.findContours(sign_red, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    mask_contours = np.zeros(sign_red.shape, sign_red.dtype)\n",
    "    \n",
    "    for contour in contours :\n",
    "        epsilon = 0.005 * cv2.arcLength(contour, True)\n",
    "        # 근사 컨투어\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, closed = True)\n",
    "        cv2.drawContours(mask_contours, [approx], -1, 255, 2)\n",
    "    \n",
    "    # 2) 객체 검출\n",
    "    _, _, stats, centroids = cv2.connectedComponentsWithStats(mask_contours)    \n",
    "    # cv2.imshow('mask_contours', mask_contours)\n",
    "    \n",
    "    for idx, centroid in enumerate(centroids) :\n",
    "        if stats[idx][0] == 0 and stats[idx][1] == 0 :\n",
    "            continue\n",
    "        if np.any(np.isnan(centroid)) :\n",
    "            continue\n",
    "            \n",
    "        x, y, w, h, area = stats[idx]\n",
    "        centerX, centerY = int(centroid[0]), int(centroid[1])\n",
    "        \n",
    "        if area<1700 and area>100 and abs(w-h)< 5 :\n",
    "            try : \n",
    "                detected_img = img[y-5 : y+h+5,  x-5 : x+w+5]\n",
    "                gray_detected_img = cv2.cvtColor(detected_img, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # 3) 허프 원 검출\n",
    "                circles = cv2.HoughCircles(gray_detected_img, cv2.HOUGH_GRADIENT, 1, 100, param1=200, param2=40, minRadius=3, maxRadius=38)\n",
    "                if circles is not None :\n",
    "                    circles = np.uint16(np.around(circles))\n",
    "                   \n",
    "                    for i in circles[0, :] :                      \n",
    "                        frame = img[y-5 : y+h+5,  x-5 : x+w+5]\n",
    "##############################################################################################                        \n",
    "                        # bgr -> rgb 로 배열순서 변경\n",
    "                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        # 리사이즈 : 64 * 64\n",
    "                        frame = cv2.resize(frame, (64, 64))\n",
    "                        \n",
    "                        # 예측\n",
    "                        preds = model.predict(np.expand_dims(frame, axis=0))[0]\n",
    "                        # 레이블 확인\n",
    "                        label = lb.classes_[np.argmax(preds)]\n",
    "##############################################################################################                        \n",
    "                        text = \"{}\".format(label)\n",
    "                        cv2.putText( img, text, ( x, y + 100 ), cv2.FONT_HERSHEY_SIMPLEX, 1.0, ( 0, 255, 0 ), 5)\n",
    "                        cv2.rectangle(img, (x-5, y-5), (x+w+5, y+h+5), (0, 0, 255), 2)\n",
    "            except : \n",
    "                continue\n",
    "    \n",
    "    ######### 2. 차선 검출 #########\n",
    "    \n",
    "    ## 2-1. 차선 ROI 생성\n",
    "    img_road = np.zeros(img.shape, img.dtype)\n",
    "    img_road[0 : img.shape[0]*11//20, : ] = img[0 : img.shape[0]*11//20 ,  : ]    \n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    cv2.imshow(video_name, img)\n",
    "    \n",
    "    if cv2.waitKey(1) == 27 :\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
